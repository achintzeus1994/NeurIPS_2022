{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rural-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from scipy.stats import multivariate_normal as mv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "molecular-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_IW_samples =10\n",
    "# m1 = 2\n",
    "# m2 = 5\n",
    "# var=5\n",
    "# x,y= sample_proposal(m1, m2, var, n_IW_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proper-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.ones(64,32)\n",
    "# y = torch.ones(10,32)\n",
    "# (x@y.T).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "silver-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input G , mu1, var1, mu2, var2\n",
    "## Output: z,W, KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baking-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "class importance_sampler():\n",
    "    def __init__(self, latent_dim1, latent_dim2, batch_size):\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "        self.batch_size = batch_size\n",
    "    def sample_proposal(self,var, n_IW_samples, device=_device):\n",
    "        mn1 = torch.distributions.MultivariateNormal(torch.zeros(self.latent_dim1), var * torch.eye(self.latent_dim1))\n",
    "        mn2 = torch.distributions.MultivariateNormal(torch.zeros(self.latent_dim2), var * torch.eye(self.latent_dim2))\n",
    "        return [mn1.sample([n_IW_samples,]).to(device), mn2.sample([n_IW_samples,]).to(device)]\n",
    "    def proposal_dist(self,z1,z2,var):\n",
    "        #cov_mat = var*torch.eye() #FIX\n",
    "        dim   = self.latent_dim1+self.latent_dim2\n",
    "        z_sqd = -(z1**2).sum(-1)-(z2**2).sum(-1)\n",
    "        p_x = 1/(2*np.pi*var)**(dim/2)*torch.exp(z_sqd/var)   #FIX add covariance \n",
    "        p_x = p_x.repeat(self.batch_size, 1)\n",
    "        return p_x\n",
    "    def target_dist(self,G,z1,z2,mu1,var1,mu2,var2):\n",
    "        # mu1: [batch_size,latent_dim1], z1: [n_IW_samples,latent_dim1]\n",
    "        g11 = G[:self.latent_dim1,:self.latent_dim2] #[latent_dim1, latent_dim2]\n",
    "        g12 = G[:self.latent_dim1,self.latent_dim2:] #[latent_dim1, latent_dim2]\n",
    "        g21 = G[self.latent_dim1:,:self.latent_dim2] #[latent_dim1, latent_dim2]\n",
    "        g22 = G[self.latent_dim1:,self.latent_dim2:] #[latent_dim1, latent_dim2] \n",
    "        z_sqd = -(z1**2).sum(-1)-(z2**2).sum(-1)     #[n_IW_samples] \n",
    "        h1   = (z1@g11*z2).sum(-1)\n",
    "        h2   = (z1@g12*(z2**2)).sum(-1)\n",
    "        h3   = ((z1**2)@g21*z2).sum(-1)\n",
    "        h4   = ((z1**2)@g22*(z2**2)).sum(-1)\n",
    "        h    = h1+h2+h3+h4      \n",
    "        d1   = (mu1@z1.T+var1@(z1**2).T)\n",
    "        d2   = (mu2@z2.T+var2@(z2**2).T)        \n",
    "        d    = d1 + d2                            #[batch_size, n_IW_samples] \n",
    "        t_x    = torch.exp((z_sqd+h+d))             #[batch_size, n_IW_samples] \n",
    "        return t_x\n",
    "    def KL_calculator(self,weights,p_x,t_x):\n",
    "        KLD = torch.tensor([1]).to(_device)\n",
    "        return KLD\n",
    "    def calc(self,G,mu1,var1,mu2,var2,n_IW_samples): \n",
    "        proposal_var = 0.1\n",
    "        x = self.sample_proposal(proposal_var,n_IW_samples)\n",
    "        z1_prior, z2_prior = self.sample_proposal(proposal_var,n_IW_samples)  #[n_IW_samples,latent_dim1],[n_IW_samples,latent_dim2]\n",
    "        z1_posterior,z2_posterior = self.sample_proposal(proposal_var,n_IW_samples)#[n_IW_samples,latent_dim1],[n_IW_samples,latent_dim2]\n",
    "        t_x_prior = self.target_dist(G,z1_prior, z2_prior,torch.zeros_like(mu1),torch.zeros_like(var1),torch.zeros_like(mu2),torch.zeros_like(var2))\n",
    "        t_x_post = self.target_dist(G,z1_posterior, z2_posterior,mu1,var1,mu2,var2)\n",
    "        p_x_prior = self.proposal_dist(z1_prior,z2_prior,proposal_var)\n",
    "        p_x_post = self.proposal_dist(z1_posterior,z2_posterior,proposal_var)  #[batch_size,n_IW_samples]\n",
    "        IS_weights_prior = torch.log(t_x_prior)-torch.log(p_x_prior) \n",
    "        prior_normalization = (torch.logsumexp(IS_weights_prior,1)).unsqueeze(1)\n",
    "        IS_weights_prior = torch.exp(IS_weights_prior - prior_normalization)\n",
    "        IS_weights_post  = torch.log(t_x_post)-torch.log(p_x_post)\n",
    "        posterior_normalization = (torch.logsumexp(IS_weights_post,1)).unsqueeze(1)\n",
    "        IS_weights_post  = torch.exp(IS_weights_post - posterior_normalization)\n",
    "        KLD = self.KL_calculator(IS_weights_post,p_x_post,t_x_post)\n",
    "        return z1_prior,z2_prior,z1_posterior,z2_posterior, IS_weights_prior,IS_weights_post, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "compact-nelson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.randn(15)\n",
    "# x = x.repeat(10, 1)\n",
    "# x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-reception",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
