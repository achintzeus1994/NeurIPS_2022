{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "secure-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from scipy.stats import multivariate_normal as mv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "corporate-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_IW_samples =10\n",
    "# m1 = 2\n",
    "# m2 = 5\n",
    "# var=5\n",
    "# x,y= sample_proposal(m1, m2, var, n_IW_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "recreational-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.ones(64,32)\n",
    "# y = torch.ones(10,32)\n",
    "# (x@y.T).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "physical-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input G , mu1, var1, mu2, var2\n",
    "## Output: z,W, KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "respective-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "class importance_sampler():\n",
    "    def __init__(self, latent_dim1, latent_dim2, batch_size):\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "        self.batch_size = batch_size\n",
    "    def sample_proposal(self,var, n_IW_samples, device=_device):\n",
    "        mn1 = torch.distributions.MultivariateNormal(torch.zeros(self.latent_dim1), var * torch.eye(self.latent_dim1))\n",
    "        mn2 = torch.distributions.MultivariateNormal(torch.zeros(self.latent_dim2), var * torch.eye(self.latent_dim2))\n",
    "        return [mn1.sample([n_IW_samples,]).to(device), mn2.sample([n_IW_samples,]).to(device)]\n",
    "    def proposal_dist(self,z1,z2,var):\n",
    "        #cov_mat = var*torch.eye() #FIX\n",
    "        dim   = self.latent_dim1+self.latent_dim2\n",
    "        z_sqd = -(z1**2).sum(-1)-(z2**2).sum(-1)\n",
    "        p_x = torch.exp(z_sqd/var)   #FIX add covariance  #1/(2*np.pi*var)**(dim/2)*\n",
    "        p_x = p_x.repeat(self.batch_size, 1)\n",
    "        return p_x\n",
    "    def target_dist(self,G,z1,z2,mu1,var1,mu2,var2):\n",
    "        # mu1: [batch_size,latent_dim1], z1: [n_IW_samples,latent_dim1]\n",
    "        g11 = G[:self.latent_dim1,:self.latent_dim2] #[latent_dim1, latent_dim2]\n",
    "        g12 = G[:self.latent_dim1,self.latent_dim2:] #[latent_dim1, latent_dim2]\n",
    "        g21 = G[self.latent_dim1:,:self.latent_dim2] #[latent_dim1, latent_dim2]\n",
    "        g22 = G[self.latent_dim1:,self.latent_dim2:] #[latent_dim1, latent_dim2] \n",
    "        z_sqd = -(z1**2).sum(-1)-(z2**2).sum(-1)     #[n_IW_samples] \n",
    "        h1   = (z1@g11*z2).sum(-1)\n",
    "        h2   = (z1@g12*(z2**2)).sum(-1)\n",
    "        h3   = ((z1**2)@g21*z2).sum(-1)\n",
    "        h4   = ((z1**2)@g22*(z2**2)).sum(-1)\n",
    "        h    = h1+h2+h3+h4      \n",
    "        d1   = (mu1@z1.T+var1@(z1**2).T)\n",
    "        d2   = (mu2@z2.T+var2@(z2**2).T)        \n",
    "        d    = d1 + d2                            #[batch_size, n_IW_samples] \n",
    "        aux = z_sqd+h+d\n",
    "#         with torch.no_grad():\n",
    "#             eps = torch.minimum(5 - aux.median(dim=-1).values, 25 - aux.max(dim=-1).values) # precision rev\n",
    "\n",
    "        print('mu1',mu1.sum())\n",
    "        print('mu2',mu2.sum())\n",
    "        print('var1',var1.sum())\n",
    "        print('var2',var2.sum())\n",
    "        print('g11',g11.sum())\n",
    "        print('g22',g22.sum())\n",
    "        print('g12',g12.sum())\n",
    "        print('g21',g21.sum())\n",
    "\n",
    "        t_x    = torch.exp(aux)             #[batch_size, n_IW_samples] \n",
    "        return t_x\n",
    "    def KL_calculator(self,weights,p_x,t_x):\n",
    "        KLD = torch.tensor([1]).to(_device)\n",
    "        return KLD\n",
    "    def calc(self,G,mu1,var1,mu2,var2,n_IW_samples): \n",
    "        proposal_var = 5\n",
    "        x = self.sample_proposal(proposal_var,n_IW_samples)\n",
    "        z1_prior, z2_prior        = self.sample_proposal(proposal_var,n_IW_samples)  #[n_IW_samples,latent_dim1],[n_IW_samples,latent_dim2]\n",
    "        z1_posterior,z2_posterior = self.sample_proposal(proposal_var,n_IW_samples)#[n_IW_samples,latent_dim1],[n_IW_samples,latent_dim2]\n",
    "        t_x_prior        = self.target_dist(G,z1_prior, z2_prior,torch.zeros_like(mu1),torch.zeros_like(var1),torch.zeros_like(mu2),torch.zeros_like(var2))\n",
    "        t_x_post         = self.target_dist(G,z1_posterior, z2_posterior,mu1,var1,mu2,var2)\n",
    "        p_x_prior        = self.proposal_dist(z1_prior,z2_prior,proposal_var)\n",
    "        p_x_post         = self.proposal_dist(z1_posterior,z2_posterior,proposal_var)  #[batch_size,n_IW_samples]\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            IS_weights_prior = torch.log(t_x_prior)-torch.log(p_x_prior) \n",
    "            prior_normalization = (torch.logsumexp(IS_weights_prior,1)).unsqueeze(1)\n",
    "            IS_weights_prior = torch.exp(IS_weights_prior - prior_normalization)\n",
    "            IS_weights_post  = torch.log(t_x_post)-torch.log(p_x_post)\n",
    "            posterior_normalization = (torch.logsumexp(IS_weights_post,1)).unsqueeze(1)\n",
    "            IS_weights_post  = torch.exp(IS_weights_post - posterior_normalization)\n",
    "            z2_posterior     = z2_posterior.unsqueeze(2).unsqueeze(3)    \n",
    "        print('t_x_prior',t_x_prior)\n",
    "        print('t_x_post',t_x_post)\n",
    "        print(torch.isinf(IS_weights_post).sum(dim=1))\n",
    "        IS_weights_post[torch.isinf(IS_weights_post)] = 0\n",
    "        print('3',IS_weights_post.sum())\n",
    "\n",
    "        KLD = self.KL_calculator(IS_weights_post,p_x_post,t_x_post)\n",
    "        return z1_prior,z2_prior,z1_posterior,z2_posterior, IS_weights_prior,IS_weights_post, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "popular-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64,20)\n",
    "y = torch.logsumexp(x,1)\n",
    "y = (y.unsqueeze(1))\n",
    "z = x - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "violent-alcohol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "latin-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(64,20)\n",
    "# y = torch.logsumexp(x,1)\n",
    "# y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "instructional-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z= (y.repeat(20,1)).T\n",
    "# z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-fighter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
